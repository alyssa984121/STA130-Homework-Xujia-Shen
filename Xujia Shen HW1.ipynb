{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7407c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_n           0\n",
       "id              1\n",
       "name            0\n",
       "gender          0\n",
       "species         0\n",
       "birthday        0\n",
       "personality     0\n",
       "song           11\n",
       "phrase          0\n",
       "full_id         0\n",
       "url             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Pick one of the datasets from the ChatBot session(s) of the TUT demo (or from your own ChatBot session if you wish) and use the code produced through the ChatBot interactions to import the data and confirm that the dataset has missing values\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5b867d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 391\n",
      "Number of columns: 11\n"
     ]
    }
   ],
   "source": [
    "#2. Start a new ChatBot session with an initial prompt introducing the dataset you're using and request help to determine how many columns and rows of data a pandas DataFrame has, and then\n",
    "#(1)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Get the number of rows and columns\n",
    "num_rows, num_columns = data.shape\n",
    "\n",
    "print(f'Number of rows: {num_rows}')\n",
    "print(f'Number of columns: {num_columns}')\n",
    "\n",
    "#(2)\n",
    "#Observations are the individual items or entries in the dataset. Each row in a dataset typically represents a single observation. \n",
    "#Variables are the pieces of information that represent characteristics or measurements of the observations, shown as the columns in dataset. \n",
    "#For example, there is a class, each student is an observation, their gender, ages, races, heights, weights are variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f2252d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for numerical columns:\n",
      "            row_n\n",
      "count  391.000000\n",
      "mean   239.902813\n",
      "std    140.702672\n",
      "min      2.000000\n",
      "25%    117.500000\n",
      "50%    240.000000\n",
      "75%    363.500000\n",
      "max    483.000000\n",
      "\n",
      "Value counts for 'personality' column:\n",
      "personality\n",
      "lazy      60\n",
      "normal    59\n",
      "cranky    55\n",
      "snooty    55\n",
      "jock      55\n",
      "peppy     49\n",
      "smug      34\n",
      "uchi      24\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for 'species' column:\n",
      "species\n",
      "cat          23\n",
      "rabbit       20\n",
      "frog         18\n",
      "squirrel     18\n",
      "duck         17\n",
      "dog          16\n",
      "cub          16\n",
      "pig          15\n",
      "bear         15\n",
      "mouse        15\n",
      "horse        15\n",
      "bird         13\n",
      "penguin      13\n",
      "sheep        13\n",
      "elephant     11\n",
      "wolf         11\n",
      "ostrich      10\n",
      "deer         10\n",
      "eagle         9\n",
      "gorilla       9\n",
      "chicken       9\n",
      "koala         9\n",
      "goat          8\n",
      "hamster       8\n",
      "kangaroo      8\n",
      "monkey        8\n",
      "anteater      7\n",
      "hippo         7\n",
      "tiger         7\n",
      "alligator     7\n",
      "lion          7\n",
      "bull          6\n",
      "rhino         6\n",
      "cow           4\n",
      "octopus       3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#3.Ask the ChatBot how you can provide simple summaries of the columns in the dataset and use the suggested code to provide these summaries for your dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Get summary statistics for numerical columns\n",
    "print(\"Summary statistics for numerical columns:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Get value counts for the 'personality' column\n",
    "print(\"\\nValue counts for 'personality' column:\")\n",
    "print(df['personality'].value_counts())\n",
    "\n",
    "# Get value counts for the 'species' column\n",
    "print(\"\\nValue counts for 'species' column:\")\n",
    "print(df['species'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b2c367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset (rows, columns): (891, 15)\n",
      "\n",
      "Column names in the dataset:\n",
      "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
      "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
      "       'alive', 'alone'],\n",
      "      dtype='object')\n",
      "\n",
      "Summary statistics for numeric columns:\n",
      "         survived      pclass         age       sibsp       parch        fare\n",
      "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
      "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
      "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
      "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
      "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
      "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
      "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n",
      "\n",
      "Summary statistics for all columns:\n",
      "          survived      pclass   sex         age       sibsp       parch  \\\n",
      "count   891.000000  891.000000   891  714.000000  891.000000  891.000000   \n",
      "unique         NaN         NaN     2         NaN         NaN         NaN   \n",
      "top            NaN         NaN  male         NaN         NaN         NaN   \n",
      "freq           NaN         NaN   577         NaN         NaN         NaN   \n",
      "mean      0.383838    2.308642   NaN   29.699118    0.523008    0.381594   \n",
      "std       0.486592    0.836071   NaN   14.526497    1.102743    0.806057   \n",
      "min       0.000000    1.000000   NaN    0.420000    0.000000    0.000000   \n",
      "25%       0.000000    2.000000   NaN   20.125000    0.000000    0.000000   \n",
      "50%       0.000000    3.000000   NaN   28.000000    0.000000    0.000000   \n",
      "75%       1.000000    3.000000   NaN   38.000000    1.000000    0.000000   \n",
      "max       1.000000    3.000000   NaN   80.000000    8.000000    6.000000   \n",
      "\n",
      "              fare embarked  class  who adult_male deck  embark_town alive  \\\n",
      "count   891.000000      889    891  891        891  203          889   891   \n",
      "unique         NaN        3      3    3          2    7            3     2   \n",
      "top            NaN        S  Third  man       True    C  Southampton    no   \n",
      "freq           NaN      644    491  537        537   59          644   549   \n",
      "mean     32.204208      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "std      49.693429      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "min       0.000000      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "25%       7.910400      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "50%      14.454200      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "75%      31.000000      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "max     512.329200      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "\n",
      "       alone  \n",
      "count    891  \n",
      "unique     2  \n",
      "top     True  \n",
      "freq     537  \n",
      "mean     NaN  \n",
      "std      NaN  \n",
      "min      NaN  \n",
      "25%      NaN  \n",
      "50%      NaN  \n",
      "75%      NaN  \n",
      "max      NaN  \n",
      "\n",
      "Count of non-null values for each numeric column:\n",
      "survived    891.0\n",
      "pclass      891.0\n",
      "age         714.0\n",
      "sibsp       891.0\n",
      "parch       891.0\n",
      "fare        891.0\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Number of columns in the dataset (from df.shape): 15\n",
      "Number of numeric columns (from df.describe()): 6\n",
      "Number of columns in full summary (from df.describe(include='all')): 15\n"
     ]
    }
   ],
   "source": [
    "#4.If the dataset you're using has (a) non-numeric variables and (b) missing values in numeric variables, explain (perhaps using help from a ChatBot if needed) the discrepancies between size of the dataset given by df.shape and what is reported by df.describe() with respect to (a) the number of columns it analyzes and (b) the values it reports in the \"count\" column\n",
    "\n",
    "#df.shape provides total number of rows and columns in the dataset, providing a complete view of its structure. \n",
    "#It includes all rows and columns, regardless of whether they contain numeric or non-numeric data or have missing values.\n",
    "\n",
    "#df.describe() provides summary statistics: count, mean, standard deviation, min, max, 25%, 50%, 75%.\n",
    "#It only includes numeric columns because it provides statistical measures that are only relevant for numerical data, such as mean and standard deviation.  \n",
    "#Non-numeric variables are excluded, and columns with missing values might not be fully represented if they have insufficient data for statistical summary.\n",
    "\n",
    "#Missing values affect the count reported by df.describe() for numeric columns. \n",
    "#When using df.describe(include='all'), the summary includes both numeric and non-numeric columns, providing additional insights into categorical data. \n",
    "#This extended summary shows unique values and frequency counts for non-numeric columns, offering a fuller picture of the dataset's attributes.\n",
    "#Both df.shape and df.describe() will reflect the same number of rows since df.describe() does not alter the rows but only provides a summary of the numeric columns.\n",
    "#df.shape includes all columns, while df.describe() includes only numeric columns. \n",
    "#Thus df.describe() will generally show fewer columns than df.shape if the dataset has non-numeric variables.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Show the shape of the dataset (number of rows and columns)\n",
    "print(\"Shape of the dataset (rows, columns):\", df.shape)\n",
    "\n",
    "# Show all column names\n",
    "print(\"\\nColumn names in the dataset:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Get summary statistics for numeric columns\n",
    "print(\"\\nSummary statistics for numeric columns:\")\n",
    "numeric_summary = df.describe()\n",
    "print(numeric_summary)\n",
    "\n",
    "# Get summary statistics for all columns (including non-numeric)\n",
    "print(\"\\nSummary statistics for all columns:\")\n",
    "all_summary = df.describe(include='all')\n",
    "print(all_summary)\n",
    "\n",
    "# Show the count of non-null values for each numeric column\n",
    "print(\"\\nCount of non-null values for each numeric column:\")\n",
    "print(numeric_summary.loc['count'])\n",
    "\n",
    "# Compare the number of columns\n",
    "print(\"\\nNumber of columns in the dataset (from df.shape):\", df.shape[1])\n",
    "print(\"Number of numeric columns (from df.describe()):\", numeric_summary.shape[1])\n",
    "print(\"Number of columns in full summary (from df.describe(include='all')):\", all_summary.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94752a7",
   "metadata": {},
   "source": [
    "5.\n",
    "\n",
    "Use your ChatBot session to help understand the difference between the following and then provide your own paraphrasing summarization of that difference\n",
    "\n",
    "Attributes provide static details about an object, showing its characteristics or properties in programming. They don't perform any actions or calculations and can be accessed directly. There is no need to use parentheses for zttributes.\n",
    "e.g.:'df.shape' is an attribute that gives the dimensions of a dataset, directly shows the number of rows and columns that the dataset has.\n",
    "\n",
    "\n",
    "Method is similar to a function applied to an object that can perform actions such as calculations or data manipulation. Methods need parentheses because they often take arguments and execute data when we run the code.\n",
    "e.g.:'df.describe()' is a method that calculates and returns summary statistics for the numerical columns in a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b578fc0",
   "metadata": {},
   "source": [
    "6.\n",
    "\n",
    "Count: The number of non-missing (non-null) values for each variable. It shows how many data points were used in the calculation.\n",
    "\n",
    "Mean: The average of the data points, calculated by summing all the values and dividing by the count. It gives an idea of the central value of the data.\n",
    "\n",
    "Std (Standard Deviation): Measures how spread out the values are from the mean. A higher standard deviation means more variability in the data.\n",
    "\n",
    "Min: The smallest value in the dataset for that variable.\n",
    "\n",
    "25% (First Quartile): The value below which 25% of the data points fall. It helps describe the lower range of the data.\n",
    "\n",
    "50% (Median): The middle value of the data when arranged in order. It divides the data into two equal halves.\n",
    "\n",
    "75% (Third Quartile): The value below which 75% of the data points fall. It is the 75th percentile of the data.It helps describe the upper range of the data.\n",
    "\n",
    "Max: The largest value in the dataset for that variable.\n",
    "\n",
    "Further questions\n",
    "\n",
    "Yes, it's clear. \n",
    "The 'df.describe()' function skips over missing values (NaNs) when calculating statistics like count, mean, standard deviation, min, max, and percentiles. Because these stats are based only on the available data, they might not fully represent the entire dataset. For instance, when values are missing, the count will be lower, making it look like there's less data. The mean could be skewed since it’s calculated from fewer points, and if the smallest or largest values are missing, the min and max won’t reflect the true range. Even percentiles might be off, as missing data can affect the spread. Essentially, if a column has lots of missing entries, summary stats like the count, mean, or median won’t give an accurate picture of the whole dataset, and the standard deviation might not capture the full variability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0880d45",
   "metadata": {},
   "source": [
    "CHAT HISTORY WITH CHATGPT\n",
    "Q1-Q6:\n",
    "\n",
    "In our conversation, we focused on analyzing two different datasets. The primary dataset you were working with was about Animal Crossing villagers, accessible from [this link](https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv). To explore this dataset, you utilized the `df.shape` attribute to determine its dimensions, revealing the number of rows and columns present. Additionally, you employed the `df.describe()` method to generate a statistical summary of the numeric columns, which includes key metrics such as `count`, `mean`, `std` (standard deviation), `min`, `25%`, `50%` (median), `75%`, and `max`.\n",
    "\n",
    "We discussed the impact of missing data on these summary statistics. Specifically, missing values are excluded from the `df.describe()` calculations. Therefore, the `count` reflects only the number of non-null entries, potentially leading to discrepancies if missing data is present. This exclusion can affect the mean, standard deviation, and other statistics, as they are computed based on the non-null values only.\n",
    "\n",
    "Furthermore, we compared attributes and methods in pandas. Attributes, such as `df.shape`, are accessed directly and provide information about the object without requiring parentheses. In contrast, methods like `df.describe()` perform computations and need to be called with parentheses to execute the action.\n",
    "\n",
    "Additionally, we analyzed another dataset, the Titanic dataset, available from [this link](https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv). We explored how to use `df.shape` and `df.describe()` to analyze this dataset and examined the differences in the number of columns reported by these functions. The `df.shape` provides the total number of rows and columns, while `df.describe()` by default includes only numeric columns in its summary. We compared this with `df.describe(include='all')`, which includes all columns, regardless of type, to provide a comprehensive overview.\n",
    "\n",
    "In summary, our discussion covered how to interpret summary statistics, the effects of missing data on these statistics, the differences between attributes and methods in pandas, and a detailed comparison of column counts using various pandas functions. This comprehensive analysis aimed to provide a clear understanding of the dataset structures and the impact of data characteristics on statistical summaries.\n",
    "\n",
    "Q1-Q6 chat history:https://chatgpt.com/share/66e37a97-4eb8-8004-8c9c-b2ee7cf2cf1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da72edd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset:\n",
      "Index(['row_n', 'id', 'name', 'gender', 'species', 'birthday', 'personality',\n",
      "       'song', 'phrase', 'full_id', 'url'],\n",
      "      dtype='object')\n",
      "\n",
      "Missing data before cleaning:\n",
      "id       1\n",
      "song    11\n",
      "dtype: int64\n",
      "\n",
      "Shape of DataFrame before cleaning:\n",
      "(391, 11)\n",
      "\n",
      "Shape of DataFrame after cleaning:\n",
      "(379, 11)\n",
      "\n",
      "Missing data after cleaning:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "#7. Missing data can be considered \"across rows\" or \"down columns\". Consider how df.dropna() or del df['col'] should be applied to most efficiently use the available non-missing data in your dataset and briefly answer the following questions in your own words\n",
    "\n",
    "#(1) \n",
    "#When analyzing survey data from a customer satisfaction study. \n",
    "#The dataset contains columns like customer id, age, satisfaction score, feedback, and purchase times. \n",
    "#The goal is to analyze the correlation between satisfaction score and purchase times.\n",
    "#Some survey are missing satisfaction scores, while all other columns are filled.\n",
    "#Using df.dropna(subset=['satisfaction scores']) to remove only the rows where the rating is missing, while keeping the rating column itself. \n",
    "# It can retain the rating column for future use and analysis.\n",
    "\n",
    "#(2) \n",
    "#when analyzing a dataset of student records for a school. \n",
    "#The dataset includes columns such as employee_id, name, program, faculty, colledge, and meal plam. \n",
    "#The meal plan column contains a lot of missing data because the school has changed the meal plan combo to a complete new version and the old version is no longer used.\n",
    "#The meal plan column is filled with mostly missing values, but you don't want to lose data in other columns.\n",
    "\n",
    "#(3)\n",
    "#Removing unnecessary columns using del df['col'] helps to simplify and clarify the dataset, making it easier for following operations. \n",
    "#Then apply df.dropna(), it runs more effectively because it focusing only on the columns that are crucial and relevant to your analysis. \n",
    "#This approach also reduces the risk of accidentally losing valuable data, which might happen if dropped rows based on less important columns.\n",
    "\n",
    "#(4)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Animal Crossing villagers dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv')\n",
    "\n",
    "# Display the column names\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Check for missing values\n",
    "missing_data_before = df.isnull().sum()\n",
    "print(\"\\nMissing data before cleaning:\")\n",
    "print(missing_data_before[missing_data_before > 0])\n",
    "\n",
    "# Drop rows with remaining missing values\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Check the shape of the DataFrame before and after cleaning\n",
    "print(\"\\nShape of DataFrame before cleaning:\")\n",
    "print(df.shape)\n",
    "\n",
    "print(\"\\nShape of DataFrame after cleaning:\")\n",
    "print(df_clean.shape)\n",
    "\n",
    "# Optionally, check missing data after cleaning\n",
    "missing_data_after = df_clean.isnull().sum()\n",
    "print(\"\\nMissing data after cleaning:\")\n",
    "print(missing_data_after[missing_data_after > 0])\n",
    "\n",
    "#Justification for the Approach\n",
    "\n",
    "#Inspecting Missing Data:\n",
    "#To identify missing values helps to see which columns and rows have incomplete data. It is important for realizing the extent of the missing data and making informed decisions on how to handle it.\n",
    "#The runs result shows that there are missing values in the 'id' and 'song' columns.\n",
    "\n",
    "# To remove irrelevant columns:\n",
    "#If columns have a lot of missing values and they are less critical for analysis, they can be removed in this step.\n",
    "#It can simplifies the dataset and avoids potential issues in the analysis due to incomplete data.\n",
    "#In this dataset, no columns were removed because the missing data was not very much.\n",
    "\n",
    "#To drop rows with remaining missing values:\n",
    "#It ensures that the dataset used for analysis is complete, rows with any missing values in the critical columns are removed. \n",
    "#This is a double check of the data.\n",
    "#The rows with missing values in the 'id' and 'song' columns were dropped. Now this is a dataset with no missing values.\n",
    "\n",
    "#\"Before and After\" Report\n",
    "\n",
    "#Before Cleaning:\n",
    "#The dataset had 11 columns: 'row_n', 'id', 'name', 'gender', 'species', 'birthday', 'personality', 'song', 'phrase', 'full_id', 'url'.\n",
    "#The 'id' column had 1 missing value, and the 'song' column had 11 missing values.\n",
    "#The dataset had 391 rows and 11 columns.\n",
    "\n",
    "#After Cleaning:\n",
    "#Shape: The dataset was reduced to 379 rows and 11 columns after cleaning.\n",
    "\n",
    "#Summary: The cleaning process involved inspecting missing data to identify columns and rows with incomplete information. Although no columns were removed due to their relevance, rows with missing values in critical columns were dropped. This resulted in a dataset that is complete and ready for further analysis, with a reduction in the number of rows but no missing values. This approach ensures that the data used for analysis is accurate and reliable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d6588",
   "metadata": {},
   "source": [
    "CHAT HISTORY WITH CHATGPT\n",
    "\n",
    "Q7\n",
    "\n",
    "In our recent discussion, we focused on handling missing data in the Animal Crossing villagers dataset from TidyTuesday. The dataset, which initially contained 391 rows and 11 columns, had missing values in the 'id' and 'song' columns. Specifically, the 'id' column had 1 missing value, while the 'song' column had 11 missing values. \n",
    "\n",
    "To address this, we first inspected the dataset to understand the distribution of missing values. This step was crucial for determining which data needed to be cleaned. We considered removing columns with excessive missing data as a typical approach, but in this case, the missing values were relatively minimal. Therefore, no columns were removed.\n",
    "\n",
    "Instead, we focused on dropping rows that contained any missing values. This approach ensured that the remaining dataset was complete and reliable for analysis. After applying this cleaning process, the dataset was reduced to 379 rows while retaining the same number of columns. Importantly, all missing data was successfully removed, resulting in a dataset with no missing values.\n",
    "\n",
    "This method of cleaning involved removing rows with missing data, ensuring that the dataset used for further analysis was accurate and complete. The cleaning process effectively addressed the missing data issue, resulting in a more reliable dataset ready for analysis.\n",
    "\n",
    "Q7 chat history: https://chatgpt.com/share/66e37a56-9c40-8004-9014-75f22ef3f005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02618eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.\n",
    "#(1)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = df.groupby(\"class\")[\"age\"].describe()\n",
    "print(summary)\n",
    "\n",
    "#df.groupby(\"class\") groups the rows in the dataset according to the \"class\" column's values.\n",
    "#select \"age\":[\"age\"]selects the \"age\" column from each group. This means that for each class group this operation focused on the age data.\n",
    "#describe() shows summary statistics for the 'age' column within each class group.\n",
    "\n",
    "#Another example other than titanic:\n",
    "#Suppose you have a dataset with the following columns:\n",
    "#characters: The character’s type\n",
    "#level: The level of the character\n",
    "#badge: The amount of badges the character has\n",
    "\n",
    "#df.groupby(\"class\")\n",
    "#It divides the data into groups based on the character’s class. So, you get two groups:\n",
    "#Group 1: Characters of the class \"Warrior\"\n",
    "#Group 2: Characters of the class \"Mage\"\n",
    "#[\"badge\"]: This focuses on the badge column within each group.\n",
    "#.describe(): This computes and displays a summary of statistics for the badge number in each class group. \n",
    "\n",
    "#For each class:\n",
    "#Count: Number of characters in each class.\n",
    "#Mean: Average badges number in each class.\n",
    "#Standard Deviation: How much thebadges number vary from the average within each class.\n",
    "#Minimum: The lowest badges number in each class.\n",
    "#25%: The badges number below which 25% of the characters fall in each class.\n",
    "#50%(Median): The middle experience points when ordered from lowest to highest in each class.\n",
    "#75%: The badges number below which 75% of the characters fall in each class.\n",
    "#Maximum: The highest badges number in each class.\n",
    "\n",
    "\n",
    "#(2)\n",
    "#'df.describe()' provides a broad summary of statistics for each numerical column in the DataFrame. It doesn’t segment the data but gives an overall view. Missing values and non-numeric columns are excluded from the calculations for metrics like mean, standard deviation, and other statistics. \n",
    "#In contrast, 'df.groupby(\"col1\")[\"col2\"].describe()' delivers a more detailed analysis within specific groups. It breaks down the statistics for column 'col2' for each category defined by 'col1'. This approach groups the data based on 'col1' and then calculates the descriptive statistics for `col2` within each group. It provides insights into how `col2`'s statistics differ across various segments defined by `col1`, offering a clearer picture of the distribution within each group.\n",
    "#With 'df.describe()', missing values impact the overall count and the statistics computed for the entire DataFrame, affecting the overall summary. \n",
    "#While 'df.groupby(\"col1\")[\"col2\"].describe()' considers missing values within each group individually. It reveals how missing values and their distribution vary among different groups, giving a more detailed understanding of how statistics of 'col2' differ from one group to another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f860b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#8\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#(3)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#A.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Wrong code:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Group by 'class' and describe the 'age' column\u001b[39;00m\n\u001b[1;32m     10\u001b[0m summary \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#8\n",
    "#(3)\n",
    "#A.\n",
    "#Wrong code:\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = df.groupby(\"class\")[\"age\"].describe()\n",
    "print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d93da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std   min   25%   50%   75%   max\n",
      "class                                                            \n",
      "First   186.0  38.233441  14.802856  0.92  27.0  37.0  49.0  80.0\n",
      "Second  173.0  29.877630  14.001077  0.67  23.0  29.0  36.0  70.0\n",
      "Third   355.0  25.140620  12.495398  0.42  18.0  24.0  32.0  74.0\n"
     ]
    }
   ],
   "source": [
    "#Q8(3)\n",
    "#A.\n",
    "#Correct code that chatgpt provides\n",
    "\n",
    "# Import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = df.groupby(\"class\")[\"age\"].describe()\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5628fb0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Group by 'class' and describe the 'age' column\u001b[39;00m\n\u001b[1;32m     11\u001b[0m summary \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "#Q8(3)\n",
    "#B. \n",
    "#WRONG CODE:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv')\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = df.groupby(\"class\")[\"age\"].describe()\n",
    "print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f7febf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std   min   25%   50%   75%   max\n",
      "class                                                            \n",
      "First   186.0  38.233441  14.802856  0.92  27.0  37.0  49.0  80.0\n",
      "Second  173.0  29.877630  14.001077  0.67  23.0  29.0  36.0  70.0\n",
      "Third   355.0  25.140620  12.495398  0.42  18.0  24.0  32.0  74.0\n"
     ]
    }
   ],
   "source": [
    "#B.\n",
    "#Correct code that Chatgpt provides:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = df.groupby(\"class\")[\"age\"].describe()\n",
    "print(summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c2427d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Group by 'class' and describe the 'age' column\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mDF\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DF' is not defined"
     ]
    }
   ],
   "source": [
    "#Q8(3)\n",
    "#C.\n",
    "#Wrong code:\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = DF.groupby(\"class\")[\"age\"].describe()\n",
    "print(summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21457536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std   min   25%   50%   75%   max\n",
      "class                                                            \n",
      "First   186.0  38.233441  14.802856  0.92  27.0  37.0  49.0  80.0\n",
      "Second  173.0  29.877630  14.001077  0.67  23.0  29.0  36.0  70.0\n",
      "Third   355.0  25.140620  12.495398  0.42  18.0  24.0  32.0  74.0\n"
     ]
    }
   ],
   "source": [
    "#C.\n",
    "#Correct code that Chatgpt provides:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = df.groupby(\"class\")[\"age\"].describe()\n",
    "print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc2a398b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (2391942147.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "#Q8(3)\n",
    "#D.\n",
    "#Wrong code\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = df.groupby(\"class\")[\"age\"].describe()\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04272989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std   min   25%   50%   75%   max\n",
      "class                                                            \n",
      "First   186.0  38.233441  14.802856  0.92  27.0  37.0  49.0  80.0\n",
      "Second  173.0  29.877630  14.001077  0.67  23.0  29.0  36.0  70.0\n",
      "Third   355.0  25.140620  12.495398  0.42  18.0  24.0  32.0  74.0\n"
     ]
    }
   ],
   "source": [
    "#Q8(3)\n",
    "#D.\n",
    "#Correct code that Chatgpt provides:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "# Note: The error was due to a missing closing parenthesis in the line below.\n",
    "# df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')  # Added the closing parenthesis\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = df.groupby(\"class\")[\"age\"].describe()\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "757ae224",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'group_by'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_211/2547532921.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Group by 'class' and describe the 'age' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'group_by'"
     ]
    }
   ],
   "source": [
    "#Q8(3)\n",
    "#E.\n",
    "#Wrong code\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = df.group_by(\"class\")[\"age\"].describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54b62af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std   min   25%   50%   75%   max\n",
      "class                                                            \n",
      "First   186.0  38.233441  14.802856  0.92  27.0  37.0  49.0  80.0\n",
      "Second  173.0  29.877630  14.001077  0.67  23.0  29.0  36.0  70.0\n",
      "Third   355.0  25.140620  12.495398  0.42  18.0  24.0  32.0  74.0\n"
     ]
    }
   ],
   "source": [
    "#Q8(3)\n",
    "#E.\n",
    "#Correct code that Chatgpt provides:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = df.groupby(\"class\")[\"age\"].describe()  # Changed 'group_by' to 'groupby'\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40433a74",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Group by 'class' and describe the 'age' column\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()  \u001b[38;5;66;03m# Changed 'group_by' to 'groupby'\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Class'"
     ]
    }
   ],
   "source": [
    "#Q8(3)\n",
    "#F\n",
    "#Wrong code:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = df.groupby(\"Class\")[\"Age\"].describe()  # Changed 'group_by' to 'groupby'\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "116aeea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std   min   25%   50%   75%   max\n",
      "class                                                            \n",
      "First   186.0  38.233441  14.802856  0.92  27.0  37.0  49.0  80.0\n",
      "Second  173.0  29.877630  14.001077  0.67  23.0  29.0  36.0  70.0\n",
      "Third   355.0  25.140620  12.495398  0.42  18.0  24.0  32.0  74.0\n"
     ]
    }
   ],
   "source": [
    "#Q8(3)\n",
    "#F\n",
    "#Correct code that Chatgpt provides:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = df.groupby(\"class\")[\"age\"].describe()  # Correct column names\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b511ca55",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4137085822.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    summary = df.groupby(class)[\"age\"].describe()  # Correct column names\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Q8(3)\n",
    "#G\n",
    "#Wrong code:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = df.groupby(class)[\"age\"].describe()  # Correct column names\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f28cd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std   min   25%   50%   75%   max\n",
      "class                                                            \n",
      "First   186.0  38.233441  14.802856  0.92  27.0  37.0  49.0  80.0\n",
      "Second  173.0  29.877630  14.001077  0.67  23.0  29.0  36.0  70.0\n",
      "Third   355.0  25.140620  12.495398  0.42  18.0  24.0  32.0  74.0\n"
     ]
    }
   ],
   "source": [
    "#Q8(3)\n",
    "#G\n",
    "#Correct code that Chatgpt provides:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Group by 'class' and describe the 'age' column\n",
    "summary = df.groupby(\"class\")[\"age\"].describe()  # Column names are in quotes\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff5c00",
   "metadata": {},
   "source": [
    "CHAT HISTROTY WITH CHATGPT\n",
    "Q8\n",
    "### Summary of Interaction\n",
    "\n",
    "#### Overview:\n",
    "The interaction focused on working with the Titanic dataset and resolving issues related to data manipulation using pandas. The primary issues involved understanding and correcting errors in code for data grouping and descriptive statistics.\n",
    "\n",
    "#### Details:\n",
    "\n",
    "1. **Initial Analysis of Titanic Dataset**:\n",
    "   - You were interested in performing initial simple summary analyses on the Titanic dataset and understanding code functionality, particularly the `df.groupby(\"col1\")[\"col2\"].describe()` method.\n",
    "\n",
    "2. **Understanding `df.groupby(\"col1\")[\"col2\"].describe()`**:\n",
    "   - This method groups the DataFrame by the column specified (`col1`), then provides descriptive statistics (like count, mean, std, min, etc.) for another column (`col2`) within each group.\n",
    "\n",
    "3. **Errors and Corrections**:\n",
    "   - **Error A**: Missing import statement.\n",
    "     - **Correction**: Added `import pandas as pd` to the code.\n",
    "   \n",
    "   - **Error B**: Incorrect URL or file path.\n",
    "     - **Correction**: Verified the correct URL for loading the dataset and corrected it.\n",
    "\n",
    "   - **Error C**: Use of incorrect method name `group_by` instead of `groupby`.\n",
    "     - **Correction**: Changed `group_by` to `groupby` for proper grouping of data.\n",
    "\n",
    "   - **Error D**: Missing closing parenthesis in `pd.read_csv`.\n",
    "     - **Correction**: Added the missing closing parenthesis to fix the syntax error.\n",
    "\n",
    "   - **Error E**: Incorrect column names and method usage.\n",
    "     - **Correction**: Used the correct column names (`'class'` and `'age'`) and proper method (`groupby()`).\n",
    "\n",
    "   - **Error G**: Syntax error due to using `class` without quotes.\n",
    "     - **Correction**: Enclosed `class` in quotes to avoid conflicts with Python reserved keywords.\n",
    "\n",
    "4. **Final Correct Code**:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "\n",
    "   # Load the dataset\n",
    "   df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "   # Group by 'class' and describe the 'age' column\n",
    "   summary = df.groupby(\"class\")[\"age\"].describe()\n",
    "   print(summary)\n",
    "   ```\n",
    "\n",
    "#### Summary:\n",
    "In the course of resolving the issues with the Titanic dataset, key aspects involved ensuring correct usage of pandas methods and column names, handling syntax errors, and understanding the `groupby` function for descriptive statistics. The corrections made addressed import statements, URL errors, method names, parentheses, and reserved keyword issues, leading to functional code for data analysis.\n",
    "\n",
    "The link of the converstion with chagpt about Q8:https://chatgpt.com/share/66e39c10-3dc4-8004-85d3-6e72b811e545"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac907a3",
   "metadata": {},
   "source": [
    "9. Mostly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
